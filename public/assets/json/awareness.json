{
    "UserAwareness": 
    [ 
        { 
            "category" : "Eye Gaze",
            "id": 1,
            "subcategory" : "",
            "visualCues" : [ "Rays", "Pointer", "Avatar" ],
            "bibliography" : [ "[NB20] NGUYEN H., BEDNARZ T.: User experience in collaborative  extended reality: Overview study. In International Conference on Virtual Reality and Augmented Reality (2020), Springer International Publishing, pp. 41-70. URL: http://dx.doi.org/10.1007/978-3-030-62655-6_3, doi:10.1007/78-3-030-62655-6_3", "[MSKB16] MASAI K., SUGIMOTO M., KUNZE K., BILLINGHURST M.: Empathy Glasses. Conference on Human Factors in Computing Systems - Proceedings 07-12-May- (2016), 1257-1263. doi:https://dl.acm.org/doi/10.1145/2851581.2892370.", "[RPM21] RIVU R., PFEUFFER K., MÜLLER P., ABDELRAHMAN Y., BULLING A., ALT F.:Altering non-verbal cues to implicitly direct attention in social vr. Association for Computing Machinery, Inc. doi:10.1145/3485279.3485309." ],
            "miniDescription": "Eye-gaze awareness enables individuals to use natural nonverbal cues to perceive where others are looking in real time.",
            "description" : "Eye-gaze awareness enables individuals to use natural nonverbal cues to perceive where others are looking in real time. Research has shown that eye-gaze tracking enhances user interaction and improves collaborative task performance [NB20]. The most commonly used visual cues for eye-gaze awareness include pointers, rays, and avatars, which help users understand their collaborators' focus. One study [MSKB16] developed a prototype that shares gaze direction using a pointer, facilitated by special glasses. Their user study found that gaze sharing not only improved collaboration but also strengthened the sense of connection among remote participants. Another study [RPM21] explored the use of avatars to enhance real-world movements with subtle redirected gazes, further enriching communication in virtual environments",
            "thumbnail" : "/assets/thumbnail/eyegaze.jpg",
            "image" : "/assets/image/EyeGaze.png"
        },

        { 
            "category" : "Face Direction",
            "id": 2,
            "subcategory" : "",
            "visualCues" : [ "Rays", "FOV Shape", "Pointer", "Orientation" ],
            "bibliography" : [ "[PLI19] PIUMSOMBOON T., LEE G. A., IRLITTI A., ENS B., THOMAS B. H., BILLINGHURST M.: On the shoulder of the giant: A multi-scale mixed reality collaboration with 360 video sharing and tangible interaction. Conference on Human Factors in Computing Systems - Proceedings, May (2019). doi:10.1145/3290605.3300458.", "[FAMR19] FRANZ J., ALNUSAYRI M., MALLOCH J., REILLY D.: Acomparative evaluation of techniques for sharing AR experiences in museums. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019), 1-20. doi:10.1145/3359226.", "[WKF19] WEISSKER T., KULIK A., FROEHLICH B.: Multi-ray jumping: Comprehensible group navigation for collocated users in immersive virtual reality. 26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings (2019), 136–144. doi:10.1109/VR.2019.8797807.", "[WZB21] WANG Z., ZHANG S., BAI X.: A mixed reality platform for assembly assistance based on gaze interaction in industry. The International Journal of Advanced Manufacturing Technology 116 (10 2021). doi:10.1007/s00170-021-07624-z" ],
            "miniDescription":"Face direction allows individuals to identify in which direction a collaborator's head is positioned.",
            "description" : "Following the same principle of natural nonverbal cues from eye gaze, the face direction allows users to identify in which direction a collaborator's head is positioned. Rays, FoV shapes, pointers, and orientation are typical visual cues employed in this face direction awareness. One relevant study [PLI19] presented CoVAR, a novel MR remote collaboration system using AR and VR technology applying eye-gaze (using rays) and head-gaze (rays mixed with FoV shape) input. They discovered that gaze cues are crucial for improving remote collaboration by reducing task load. Further, a group of researchers [FAMR19] implemented a prototype with a ray emanating from the user's head direction. Interestingly, the ray was also useful for selection purposes. In a teleport task, in which one user brings another one into a collaborative jump, orientation cues are used to avoid motion sickness. The investigators [WKF19] developed a window and a compass (orientation cues) to limit jumping based on the navigator's viewing direction. Pointer cues were used in a study showing that sharing head pointers can improve performance, co-presence awareness, and user collaborative experiences [WZB21].",
            "thumbnail" : "/assets/thumbnail/facedirection.jpg",
            "image" : "/assets/image/faceDirection.png"
        },

        { 
            "category" : "User Perspective and Frustum",
            "id": 3,
            "subcategory" : "",
            "visualCues" : [ "Point of View", "Scale", "FOV Shape" ],
            "bibliography" : [ "[PDE19] PIUMSOMBOON T., DEY A., ENS B., LEE G., BILLINGHURST M.: The effects of sharing awareness cues in collaborative mixed reality. Frontiers Robotics AI 6, FEB (2019), 1-18. doi:10.3389/frobt.2019.00005.", "[PLLB17] PIUMSOMBOON T., LEE Y., LEE G., BILLINGHURST M.: CoVAR: A collaborative virtual and augmented reality system for remote collaboration. SIGGRAPH Asia 2017 Emerging Technologies, SA2017, November (2017). doi:10.1145/3132818.3132822.", "[XHPW18] XIA H., HERSCHER S., PERLIN K., WIGDOR D.: Space-Time: Enabling fluid individual and collaborative editing in virtual reality. UIST 2018 - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology, October 2018 (2018), 853-866.doi:10.1145/3242587.3242597." ],
            "miniDescription":"What users are looking at.",
            "description" : "This awareness concerns knowing what others are looking at during the moment of the interaction. This awareness gives feedback on the direction and the area of another view range, enhancing the context and communication of the target subject. One research [PDE19] prototyped a visual cue based on FoV shape to indicate the individual's view range and direction. Also, it is possible to see through another collaborator's eyes (using the point of view cue) in a technique called AV-Snap-to-VR created by [PLLB17]. The mirrored user can snap to another user's head with an independent head orientation and minor control of their head position offset to avoid simulation sickness. Other authors [XHPW18] designed another innovative example of user perspective by manipulating the avatar scale. Users may change their size by scaling down their avatars so that others would be aware of these users' perspectives in a scene. Hence, the found cues were the point of view, scale, and FoV shape.",
            "thumbnail" : "/assets/thumbnail/userperspective.jpg",
            "image" : "/assets/image/userPerspective.png"
        },

        { 
            "category" : "User Position",
            "id": 4,
            "subcategory" : "",
            "visualCues" : [ "Shadows", "Orientation" ],
            "bibliography" : [ "[SMF15] SOUSA M., MENDES D., FERREIRA A., PEREIRA J. M., JORGE J.: Eery Space: Facilitating Virtual Meetings Through Remote Proxemics. In 15th IFIP TC 13 International Conference on Human-Computer Interaction — INTERACT 2015 - Volume 9298 (Berlin, Heidelberg, 2015), Springer-Verlag, pp. 622-629. URL:https://doi.org/10.1007/978-3-319-22698-9_43,", "[SMM17] SOUSA M., MENDES D., MEDEIROS D., FERREIRA A., PEREIRA J. M., JORGE J.: Remote proxemics. Collaboration Meets Interactive Spaces (2017), 47-73. doi:10.1007/978-3-319-45853-3_4" ],
            "miniDescription":"The awareness of the individual's location in the space.",
            "description" : "The awareness of the individual's location in the space during a collaborative experience. Similar to realworld scenarios where a person's position signifies their proximity to collaborators, different communication actions are required based on the distance between individuals. Interacting with closer users is inherently more straightforward than with those farther away. Shadows serve as cues, indicating the distance from a person to a wall and providing a sense of the spatial relationship between them [SMF15]. Additionally, visual cues such as bubble maps on mobile phones are employed to present user positions within the environment, enhancing awareness of individual locations [SMM17].",
            "thumbnail" : "/assets/thumbnail/userposition.jpg",
            "image" : "/assets/image/userPosition.png"
        },

        { 
            "category" : "User Identification",
            "id": 5,
            "subcategory" : "",
            "visualCues" : [ "Avatar", "Text" ],
            "bibliography" : [ "[ZSM19] ZORZAL E. R., SOUSA M., MENDES D., DOS ANJOS R. K., MEDEIROS D., PAULO S. F., RODRIGUES P., MENDES J. J., DELMAS V., UHL J. F., MOGORRÓN J., JORGE J. A., LOPES D. S.: Anatomy Studio: A tool for virtual dissection through augmented 3D reconstruction. Computers and Graphics (Pergamon) 85 (2019), 74-84. doi:10.1016/j.cag.2019.09.006." ],
            "miniDescription":"Help users to identify other users in a collaborative session.",
            "description" : "Its main purpose is to help users to identify other users. So, to achieve that, avatars and text are the experienced visual cues more commonly found in the literature. A prototype developed by [ZSM19] projected an avatar during an interaction so users could recognize who they were talking to with no additional information. Nevertheless, when projecting personalized avatars is not implemented, text cues are helpful to identify users.",
            "thumbnail" : "/assets/thumbnail/useridentification.jpg",
            "image" : "/assets/image/userIdentification.png"
        },

        { 
            "category" : "Intention",
            "id": 6,
            "subcategory" : "Teleport",
            "visualCues" : [ "Rays", "Virtual Volume", "Ghosting" ],
            "bibliography" : [ "[WKF19] WEISSKER T., KULIK A., FROEHLICH B.: Multi-ray jumping: Comprehensible group navigation for collocated users in immersive virtual reality. 26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings (2019), 136-144. doi:10.1109/VR.2019.8797807.", "[XHPW18] XIA H., HERSCHER S., PERLIN K., WIGDOR D.: Space-Time: Enabling fluid individual and collaborative editing in virtual reality. UIST 2018 - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology, October 2018 (2018), 853-866. doi:10.1145/3242587.3242597." ],
            "miniDescription":"Intention to move long or short distances.",
            "description" : "Teleporting is a navigation method in which users change spatial positions, skipping the transition between the start and final destination. It enables users to move long distances virtually when the physical space is smaller than the virtual world. Rays, virtual volume, and ghosting are visual cues that create awareness of teleport intention. A found study [WKF19] investigated the action of teleporting using rays in a collaborative task. They implemented a prototype that allows users to bring another collaborator to a new position by teleporting together. So, they have created three variations of a multi-ray jumping technique, consisting of a secondary ray for the passive user to be aware of the intention to teleport. Otherwise, they would feel disoriented and suffer from motion sickness. Thus, the teleport intention cues are proposed to warn a user that a collaborator will change position and bring this passive user to a new location in the virtual world. One good example of using ghosting and virtual volumes to indicate an intended teleport task from another user was developed by [XHPW18]. Individuals may create and position another user's avatar with different dimensions to create an awareness of the teleport destination for that user. This special avatar is denoted as a parallel avatar and has a ghosting aspect. In addition, passive travelers get a notification with a virtual volume close to their eyes, indicating the intention of another user to communicate in another location. To teleport, users need to select using a controller on this virtual volume and jump to the final destination.",
            "thumbnail" : "/assets/thumbnail/intentionteleport.jpg",
            "image" : "/assets/image/intentionTeleport.png"
        },

        { 
            "category" : "Intention",
            "id": 7,
            "subcategory" : "Iteration Proposal",
            "visualCues" : [ "Ghosting" ],
            "bibliography" : [ "[PMR19] PEREIRA V., MATOS T., RODRIGUES R., NOBREGA R., JACOB J.: Extended reality framework for remote collaborative interactions in virtual environments. ICGI 2019 - Proceedings of the International Conference on Graphics and Interaction (2019), 17-24. doi:10.1109/ICGI47575.2019.8955025." ],
            "miniDescription":"Intention to propose a design iteration on an object.",
            "description" : "Scenes may need to be rearranged with new design versions or iterations, including positioning, scaling, and rotating operations. So, aiming at not losing the original state, users can be aware of new designs proposed by other collaborators using ghosting cues. This awareness is developed by [PMR19] in which it is possible to compare two object versions simultaneously (side by side).",
            "thumbnail" : "/assets/thumbnail/intentioniteration.jpg",
            "image" : "/assets/image/intentionIterationProposal.png"
        },

        { 
            "category" : "Intention",
            "id": 8,
            "subcategory" : "User-User Interaction",
            "visualCues" : [ "Highlight" ],
            "bibliography" : [ "[SMM17] SOUSA M., MENDES D., MEDEIROS D., FERREIRA A., PEREIRA J. M., JORGE J.: Remote proxemics. Collaboration Meets Interactive Spaces (2017), 47-73. doi:10.1007/978-3-319-45853-3_4." ],
            "miniDescription":"Intention to start a communication with other users.",
            "description" : "Starting communication with other users may not be clear in an immersive environment due to a lack of body representation or facial expressions. Thus, calling the attention of collaborators to start an interaction is the main intention of this user awareness. Highlighting is the usual cue observed in the literature. For instance, [SMM17] developed a mechanism called bubble maps, which project shapes on the floor indicating an intention to talk. So, these shapes are highlighted based on people's proximity, pointing out the users' interaction intentions.",
            "thumbnail" : "/assets/thumbnail/intentioninteractions.jpg",
            "image" : "/assets/image/intentionUserUserInteraction.png"
        },

        { 
            "category" : "User-User Relations",
            "id": 9,
            "subcategory" : "Interactions",
            "visualCues" : [ "Highlight" ],
            "bibliography" : [ "[SMM17] SOUSA M., MENDES D., MEDEIROS D., FERREIRA A., PEREIRA J. M., JORGE J.: Remote proxemics. Collaboration Meets Interactive Spaces (2017), 47-73. doi:10.1007/978-3-319-45853-3_4." ],
            "miniDescription":"It is the awareness when users interact with others during a task.",
            "description" : "It is the awareness when users interact with others during a task. Typically, users should be aware if a collaborator is talking or presenting something to others. The typical cues found in the literature are highlights. A prototype [SMM17] highlights a group when they are closer and start socializing, making other people aware of that social interaction.",
            "thumbnail" : "/assets/thumbnail/useruserinteraction.jpg",
            "image" : "/assets/image/userUserRelationsInteractions.png"
        },

        { 
            "category" : "User-User Relations",
            "id": 10,
            "subcategory" : "Non-verbal Communication",
            "visualCues" : [ "Facial Expressions", "Hands" ],
            "bibliography" : [ "[MSKB16] MASAI K., SUGIMOTO M., KUNZE K., BILLINGHURST M.: Empathy Glasses. Conference on Human Factors in Computing Systems - Proceedings 07-12-May- (2016), 1257-1263. doi:https://dl.acm.org/doi/10.1145/2851581.2892370.", "[HKBA19] HUANG W., KIM S., BILLINGHURST M., ALEM L.: Sharing hand gesture and sketch cues in remote collaboration. Journal of Visual Communication and Image Representation 58 (2019), 428-438. URL:https://doi.org/10.1016/j.jvcir.2018.12.010, doi:10.1016/j.jvcir.2018.12.010." ],
            "miniDescription":"Nonverbal communication signals that humans use to communicate.",
            "description" : "It is nonverbal communication signals that humans use to communicate more naturally. Gestures and facial expressions are typical fundamental components of language that contribute to a spoken message. A relevant paper [MSKB16] investigated a novel technique to explore how effective the empathy glasses were at supporting remote collaboration by using facial expression cues. In addition, hands are familiar cues to express human gestures during a conversation. [HKBA19] presented a study that showed the benefits of using hands for remote guidance with sketching cues for more complex collaborative tasks.",
            "thumbnail" : "/assets/thumbnail/userusernonverbal.jpg",
            "image" : "/assets/image/userUserRelationsNonVerbal.png"
        },

        { 
            "category" : "User-Object Relations",
            "id": 11,
            "subcategory" : "Selection and Manipulation",
            "visualCues" : [ "Highlight", "Hands" ],
            "bibliography" : [ "[XHPW18] XIA H., HERSCHER S., PERLIN K., WIGDOR D.: Space-Time: Enabling fluid individual and collaborative editing in virtual reality. UIST 2018 - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology, October 2018 (2018), 853-866. doi:10.1145/3242587.3242597." ],
            "miniDescription":"Users select or manipulate shared objects in a virtual world.",
            "description" : "When users select or manipulate shared objects in a virtual world, collaborators should be aware of that information, e.g., to avoid editing the same mesh simultaneously when this is not possible. Also, this awareness clarifies which tasks are being executed by individual users. In the Spacetime prototype [XHPW18], users can reach objects using rays to brush over a set of objects. Once selected, these objects have their wireframe highlighted or collected inside a container, which can be manipulated to move or scale objects. Another visual cue commonly used is the hands, which can be digitally represented and are crucial to indicate when a user is reaching to grab an object, for example.",
            "thumbnail" : "/assets/thumbnail/userobjectselect.jpg",
            "image" : "/assets/image/userObjectRelationsSelect.png"
        },

        { 
            "category" : "User-Object Relations",
            "id": 12,
            "subcategory" : "Simultaneous Select and Manipulation",
            "visualCues" : [ "Ghosting" ],
            "bibliography" : [ "[XHPW18] XIA H., HERSCHER S., PERLIN K., WIGDOR D.: Space-Time: Enabling fluid individual and collaborative editing in virtual reality. UIST 2018 - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology, October 2018 (2018), 853-866. doi:10.1145/3242587.3242597." ],
            "miniDescription":"Users select or manipulate at the same time shared objects in a virtual world.",
            "description" : "Objects in a virtual environment can be selected and manipulated in some collaborative systems. When these options are available, it is important to warn other users that one collaborator holds a specific object. That avoids unexpected changes in the scene and a lack of context during these tasks. Ghosting is the visual cue used to generate this awareness. For instance, [XHPW18] denoted a concept called parallel object, in which users can select and manipulate the same object. So, to alert users they have chosen an already taken object, the system changes the object's property, giving an aspect of a ghost.",
            "thumbnail" : "/assets/thumbnail/userobjectsimultaneous.jpg",
            "image" : "/assets/image/userObjectRelationSimulatenousSelect.png"
        },

        { 
            "category" : "User-Object Relations",
            "id": 13,
            "subcategory" : "Annotation",
            "visualCues" : [ "Sketching" ],
            "bibliography" : [ "[WBB20a] WANG P., BAI X., BILLINGHURST M., ZHANG S., HAN D., SUN M., WANG Z., LV H., HAN S.: Haptic Feedback Helps Me? A VR-SAR Remote Collaborative System with Tangible Interaction. International Journal of Human-Computer Interaction 36, 13 (2020), 1242-1257. URL: https://doi.org/10.1080/10447318.2020.1732140, doi:10.1080/10447318.2020.1732140." ],
            "miniDescription": "Communicate attaching annotations on scene objects.",
            "description" : "It is a way of communication among users attached to a scene object. Annotations are crucial in asynchronous collaboration by conveying messages relevant to a specific object. Notably, [WBB20a] researched sharing virtual non-verbal sketching cues to enhance remote collaboration. They have developed the concept of shared AR Annotations (ARAs), which can enhance co-presence awareness.",
            "thumbnail" : "/assets/thumbnail/userobjectannotation.jpg",
            "image" : "/assets/image/userObjectRelationAnnotation.png"
        },

        { 
            "category" : "User-Object Relations",
            "id": 14,
            "subcategory" : "Interest Zone",
            "visualCues" : [ "Rays", "Hands", "Pointer", "Symbol", "Virtual Volume" ],
            "bibliography" : [ "[AKK11] ARGELAGUET F., KULIK A., KUNERT A., ANDUJAR C., FROEHLICH B.: See-through techniques for referential awareness in collaborative virtual reality. International Journal of Human Computer Studies 69, 6 (2011), 387-400. doi:10.1016/j.ijhcs.2011.01.003.", "[CLL21] CHEN L., LIU Y., LI Y., YU L., GAO B., CAON M., YUE Y., LIANG H.-N.: Effect of Visual Cues on Pointing Tasks in Co-Located Augmented Reality Collaboration. In Symposium on Spatial User Interaction (New York, NY, USA, 2021), SUI 21, Association for Computing Machinery. URL: https://doi.org/10.1145/3485279.3485297, doi:10.1145/3485279.3485297.", "[SMD19] SOUSA M., MENDES D., DOS ANJOS R. K., LOPES D. S., JORGE J.: Negative space: Workspace awareness in 3D face-to-face remote collaboration. Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry (2019). doi:10.1145/3359997.3365744.", "[ZBZ∗23] ZHANG X., BAI X., ZHANG S., HE W., WANG S., YAN Y., YU Q., LIU L.: A novel mr remote collaboration system using 3d spatial area cue and visual notification. Journal of Manufacturing Systems 67 (4 2023), 389-409. doi:10.1016/j.jmsy.2023.02.013.", "[WZW20] WOLFARTSBERGER J., ZENISEK J., WILD N.: Supporting teamwork in industrial virtual reality applications. Procedia Manufacturing 42, 2019 (2020), 2–7. URL: https://doi.org/10.1016/j.promfg.2020.02.016, doi:10.1016/j.promfg.2020.02.016." ],
            "miniDescription":"Context area or zone that makes collaboration task clearer.",
            "description" : "During collaborative tasks, it can be critical to point out an object or an area to create context and make the collaboration task clearer. This is mainly observed in a real-world situation where individuals use hand gestures to point something to another person during a conversation. It is a natural behavior that might also happen in an immersive environment but with more possibilities due to the extension of technology. A good example is the possibility of pointing and seeing occluded objects by creating a transparency area (transparency technique) or by cutting a region away (cutaway technique) using rays and pointers cues [AKK11]. [CLL21] studied another use of rays, where they compared the usability of pointing lines versus moving tracks techniques to point out objects before selecting them. Their results showed that pointing lines had a better performance and ease of control over motion tracks. Also, the interest zone identification can be facilitated by adjusting the user's finger direction in a virtual environment [SDM19]. This technique, called warping deixis, is useful for distant targets in which observers frequently misinterpret the target of a pointing gesture. Virtual volumes were also a visual cue used as spatial area volume [ZBZ23]. According to the study, they have contributed to a faster task performance time. Finally, a prototype created by [WZW20] allowed users to leave symbols in the form of a flare on the object's surface to indicate an interesting zone to other collaborators.",
            "thumbnail" : "/assets/thumbnail/userobjectinterestzone.jpg",
            "image" : "/assets/image/userObjectRelationInterestZone.png"
        },

        { 
            "category" : "User Status",
            "id": 15,
            "subcategory" : "",
            "visualCues" : [ "Virtual Volume" ],
            "bibliography" : [ "[PBHM19] PETRYKOWSKI M., BERGER P., HENNIG P., MEINEL C.: Digital Collaboration with a Whiteboard in Virtual Reality. In Proceedings of the Future Technologies Conference (FTC) 2018 (Cham, 2019), Arai K., Bhatia R., Kapoor S., (Eds.), Springer International Publishing, pp. 962–981." ],
            "miniDescription":"The current status of a user.",
            "description" : "It indicates and shares the current status of a user with collaborators. The status could indicate different information like the lack of sound (user is not listening) or a concentration requirement (user is focused on a specific task) as demonstrated in a study conducted by [PBHM19]. They have added a virtual volume cue to aid users in being aware of the status of others. So, placing people into a virtual environment shield made them focus on the task without being disturbed.",
            "thumbnail" : "/assets/thumbnail/userstatus.jpg",
            "image" : "/assets/image/userStatus.png"
        },

        { 
            "category" : "User Active Tool",
            "id": 16,
            "subcategory" : "",
            "visualCues" : [ "Symbol" ],
            "bibliography" : [ "[WBB20b] WOODWORTH J. W., BROUSSARD D., BORST C. W.: Designing tools to improve collaborative interaction in a vr environment for teaching geosciences interpretation. Gesellschaft fur Informatik (GI). doi:10.18420/muc2020-ws122-326." ],
            "miniDescription":"Indicates and shares the active tool a user is holding.",
            "description" : "It indicates and shares the active tool a user is holding. Before interacting with the scene, a tool icon or a symbol cue can be shown over the user avatar to indicate which active tool users are using [WBB20b].",
            "thumbnail" : "/assets/thumbnail/useractivetool.jpg",
            "image" : "/assets/image/userActiveTool.png"
        },

        { 
            "category" : "User Roles and Abilities",
            "id": 17,
            "subcategory" : "",
            "visualCues" : [ "Symbol", "Avatar" ],
            "bibliography" : [ "[WF21] WEISSKER T., FROEHLICH B.: Group Navigation for Guided Tours in Distributed Virtual Environments. IEEE Transactions on Visualization and Computer Graphics 27, 5 (2021), 2524-2534. doi:10.1109/TVCG.2021.3067756.", "[WZW20] WOLFARTSBERGER J., ZENISEK J., WILD N.: Supporting teamwork in industrial virtual reality applications. Procedia Manufacturing 42, 2019 (2020), 2-7. URL: https://doi.org/10.1016/j.promfg.2020.02.016, doi:10.1016/j.promfg.2020.02.016." ],
            "miniDescription":"Indicate different roles and interaction possibilities from users.",
            "description" : "In a group session, individuals may have different roles and interaction possibilities. One way to alert users of a role is using symbol cues to identify their roles quickly. A significant example was developed by [WF21], which investigated a group navigation theme from a tour perspective. To help users identify the tour guide among all people, researchers have added a crown symbol cue on top of the guide avatar so other individuals would be aware of that user role. Also, in a paper authored by [WZW20], users' avatars were substituted with virtual reality (VR) glasses to symbolize participants who were engaged in augmented reality (AR) sessions rather than VR. This visual representation effectively conveyed an asymmetrical setup to all collaborators.",
            "thumbnail" : "/assets/thumbnail/userrole.jpg",
            "image" : "/assets/image/userRoles.png"
        },

        { 
            "category" : "User Presence",
            "id": 18,
            "subcategory" : "",
            "visualCues" : [ "Avatar", "Shadows", "Full-body Avatar" ],
            "bibliography" : [ "[SMM17] SOUSA M., MENDES D., MEDEIROS D., FERREIRA A., PEREIRA J. M., JORGE J.: Remote proxemics. Collaboration Meets Interactive Spaces (2017), 47-73. doi:10.1007/978-3-319-45853-3_4." ],
            "miniDescription":"Make users aware that other collaborators are present in the same session.",
            "description" : "In synchronous collaboration, when more than one user is engaged, it is essential to make users aware that other collaborators are present at the exact moment. Avatars and fullbody avatars are the systems' most common visual cues to indicate users' presence. Also, shadows on a wall can project users on a screen, indicating their presence in a prototype developed by [SMM17].",
            "thumbnail" : "/assets/thumbnail/userpresence.jpg",
            "image" : "/assets/image/userPresence.png"
        },

        { 
            "category" : "User Embodiment",
            "id": 19,
            "subcategory" : "",
            "visualCues" : [ "Hands", "Full-body Avatar", "Controllers" ],
            "bibliography" : [ "[ELT19] ENS B., LANIR J., TANG A., BATEMAN S., LEE G., PIUMSOMBOON T., BILLINGHURST M.: Revisiting collaboration through mixed reality: The evolution of groupware. International Journal of Human Computer Studies 131 (2019), 81-98. doi:10.1016/j.ijhcs.2019.05.011.", "[WWJ19] WU Y., WANG Y., JUNG S., HOERMANN S., LINDEMAN R. W.: Exploring the use of a robust depth-sensor-based avatar control system and its effects on communication behaviors. Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST (2019). doi:10.1145/3359996.3364267." ],
            "miniDescription":"It represents the user's body in a digital space.",
            "description" : "It represents the user's body in a digital space, addressing the user's representation of physicality in remote work [ELT19]. The represented body parts can enhance natural non-verbal communication (poses and gestures) between users, leveraging collaboration. [WWJ19] presented one interesting example using full-body avatars (depth-sensor-based) to depict users. With three different avatar control conditions developed (first-person, third-person, and a real-world view), participants indicated better performance in the communication scenario regarding non-verbal behavior cues. Also, hands are a simple cue to denote this particular part of the body, but sometimes, they are not added to the system, which includes a pair of controller cues. Because they are attached to human hands, controllers can represent this body part and be used as a hand in a collaboration task.",
            "thumbnail" : "/assets/thumbnail/userembodiment.jpg",
            "image" : "/assets/image/userEmbodiment.png"
        }
    ]


}